{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d4788b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import lxml\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "def scrape_data(url):\n",
    "    \"\"\"\n",
    "    A script to scrape data from given url.\n",
    "\n",
    "    Keyword arguments:\n",
    "    url: url link to the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    #driver = webdriver.Chrome()\n",
    "    #driver_path = 'Add the path to chrome driver here'\n",
    "    driver_path = 'C:/Users/David Ugochi Asogwa/Documents/Folders/GitHub/Web_Scrapping_Motor_Homes/chromedriver.exe'\n",
    "    driver = webdriver.Chrome(service = Service(driver_path))\n",
    "    \n",
    "    # Get URL\n",
    "    # Select RV class and fuel type: Diesel\n",
    "    driver.get(url)\n",
    "    rv_class = driver.find_element(By.XPATH, '//*[@id=\"rv_class\"]/fieldset/div[1]').click()\n",
    "    time.sleep(1)\n",
    "    fuel_type = driver.find_element(By.XPATH, '//*[@id=\"classAFuel\"]/div[3]/label').click()\n",
    "\n",
    "    # List to hold extracted data.\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through pages till end.\n",
    "    # Get current page source and container frames.\n",
    "    # loop through all item in the contaimer frames collecting required data.\n",
    "\n",
    "    while True:\n",
    "\n",
    "        time.sleep(1)\n",
    "        search_body = bs(driver.page_source, 'lxml')\n",
    "        container_frames = re.findall('id=\"pagination_container_\\w+\"', str(search_body))\n",
    "\n",
    "        for container in container_frames:\n",
    "            time.sleep(1)\n",
    "            vehicle_name = driver.find_element(By.XPATH, '//*[@' + str(container)\n",
    "                                               + ']/div/div[3]/div[1]/div[1]/a/span').text\n",
    "            status = driver.find_element(By.XPATH, '//*[@' + str(container)\n",
    "                                         + ']/div/div[3]/div[1]/div[1]/span').text.capitalize()\n",
    "            location = driver.find_element(By.XPATH, '//*[@' + str(container) + ']/div/div[3]/div[1]/div[2]/span[1]').text\n",
    "            stock_number = driver.find_element(By.XPATH, '//*[@'\n",
    "                                               + str(container)\n",
    "                                               + ']/div/div[3]/div[1]/div[2]/span[2]').text.split(' ')[2]\n",
    "            length = driver.find_element(By.XPATH, '//*[@' + str(container) + ']//div[@class=\"specs\"]').text.split('\\n')[1]\n",
    "            sleeps = driver.find_element(By.XPATH, '//*[@' + str(container)\n",
    "                                         + ']//div[@class=\"specs\"][2]').text.split('\\n')[1]\n",
    "            try:\n",
    "                sales_price = driver.find_element(By.XPATH, '//*[@'\n",
    "                                                  + str(container) \n",
    "                                                  + ']//span[@class=\"price-info low-price \"]').text[1:].replace(',', '')\n",
    "            except NoSuchElementException:\n",
    "                sales_price = driver.find_element(By.XPATH, '//*[@'\n",
    "                                                  + str(container)\n",
    "                                                  + ']//span[@class=\"price-info low-price\"]').text[1:].replace(',', '')\n",
    "\n",
    "            try:\n",
    "                if int(sales_price) > 300000:\n",
    "                    get_details = driver.find_element(By.XPATH, '//*[@'+str(container)+']/div/div[3]/div[4]/a[2]').click()\n",
    "                    time.sleep(2)\n",
    "                    get_page_source = bs(driver.page_source, 'lxml')\n",
    "                    tab_content = get_page_source.find('div', {'class':'tab-content'}).find_all('div',\n",
    "                                                                                         {'class':'oneSpec clearfix'})\n",
    "                    specifications_1 = [spec.find('h4').text for spec in tab_content]\n",
    "                    specifications_2 = [spec.find('h5').text for spec in tab_content]\n",
    "                    specifications = dict(zip(specifications_1, specifications_2))\n",
    "                    horse_power = specifications['HORSEPOWER']\n",
    "                    back_to_results = driver.find_element(By.ID, 'back-link').get_attribute('href')\n",
    "                    driver.get(back_to_results)\n",
    "\n",
    "                else:\n",
    "                    horse_power = 'N/A'\n",
    "\n",
    "            except KeyError:\n",
    "                horse_power = 'N/A'\n",
    "\n",
    "                # Return to search results.\n",
    "                time.sleep(1)\n",
    "                back_to_results = driver.find_element(By.ID, 'back-link').get_attribute('href')\n",
    "                driver.get(back_to_results)\n",
    "\n",
    "            # Append extracted data to list of dictionaries.\n",
    "            df_list.append({'vehicle_name': vehicle_name,\n",
    "                            'stock_number': stock_number,\n",
    "                            'status': status,\n",
    "                            'location': location,\n",
    "                            'sleeps': sleeps,\n",
    "                            'length': length,\n",
    "                            'sales_price (USD)': int(sales_price),\n",
    "                            'horse_power': horse_power})\n",
    "\n",
    "        # Go to next page.\n",
    "        try:\n",
    "            next_page = driver.find_element(By.XPATH, '//*[@id=\"page_next\"]').click()\n",
    "        except NoSuchElementException:\n",
    "            break         # Break at the end of the page.\n",
    "\n",
    "    # Close driver.\n",
    "    driver.close()\n",
    "\n",
    "    # Transform extracted data into a data frame.\n",
    "    df = pd.DataFrame(df_list, columns = ['vehicle_name', 'stock_number', 'status', 'location','fuel_type', 'sleeps',\n",
    "                                          'length', 'sales_price (USD)', 'horse_power'])\n",
    "\n",
    "    #df.to_csv('RV_MotorHomes_with_possible_duplicates.csv', index = False)\n",
    "    \n",
    "    # Drop duplicates from the data set.\n",
    "    df.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Save dataframe as CSV.\n",
    "    df.to_csv('RV_MotorHomes.csv', index = False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #url = 'https://rv.campingworld.com/rvclass/motorhome-rvs'\n",
    "#     url = str(input('Enter URL: '))\n",
    "#     scrape_data(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e470dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
